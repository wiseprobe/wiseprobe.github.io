---
layout: post
title: A Handy Python Toolkit for LLMs
categories: [Blog]
tags: [llm,generative AI,toolkits]
---

Not long after the release of OpenAI’s ChatGPT, data security vendor Cyberhaven observed that employees across industries were sharing sensitive and privacy-protected information while interacting with the large language model.  Within the defense industry, usage of such external services poses a significant security risk. However, since the release of open large language models from organizations like Meta and MistralAI, it is now possible to efficiently run ChatGPT-like models on your own machines using non-public data, without sharing information externally.

The [onprem](https://github.com/amaiya/onprem) Python package makes it easier to apply large language models on-premises to non-public data.  This allows to run ChatGPT-like large language models behind corporate firewalls and within air-gapped networks with no risk of data leakage.

After the package is [installed](https://amaiya.github.io/onprem/#install), you can load an LLM as follows:

```python
from onprem import LLM
llm = LLM(n_gpu_layers=-1)
```

As of this writing, the default LLM as a 7B-parameter Mistral model, but you can [load any available LLM ](https://amaiya.github.io/onprem/#faq)of your choosing. This is especially useful if the default LLM struggles with a particular task.

Let's run through some quick examples of using LLMs to solve various tasks.

## Information Extraction

```python
prompt = """ Extract the Name, Current Position, and Current Company from each piece of Text.

Text: Alan F. Estevez serves as the Under Secretary of Commerce for Industry and Security.  As Under Secretary, Mr. Estevez leads
the Bureau of Industry and Security, which advances U.S. national security, foreign policy, and economic objectives by ensuring an
effective export control and treaty compliance system and promoting U.S. strategic technology leadership.
A: Name:  Alan F. Estevez | Current Position: Under Secretary | Current Company: Bureau of Industry and Security

Text: Pichai Sundararajan (born June 10, 1972[3][4][5]), better known as Sundar Pichai (/ˈsʊndɑːr pɪˈtʃaɪ/), is an Indian-born American
business executive.[6][7] He is the chief executive officer (CEO) of Alphabet Inc. and its subsidiary Google.[8]
A: Name:   Sundar Pichai | Current Position: CEO | Current Company: Google

Text: Norton Allan Schwartz (born December 14, 1951)[1] is a retired United States Air Force general[2] who served as the 19th Chief of Staff of the
Air Force from August 12, 2008, until his retirement in 2012.[3] He previously served as commander, United States Transportation Command from
September 2005 to August 2008. He is currently the president of the Institute for Defense Analyses, serving since January 2, 2020.[4]
A:"""
saved_output = llm.prompt(prompt, stop=['\n\n'])

# OUTPUT
#  Name: Norton Allan Schwartz | Current Position: President | Current Company: Institute for Defense Analyses
```

In the example above, we supplied the input text as part of the prompt. You can also perform information extraction by pointing to a file directly:

```python
extractor = Extractor(llm)
prompt = """Extract the names of research institutions (e.g., universities, research labs, corporations, etc.)
from the following sentence delimitated by three backticks. If there are no organizations, return NA.
If there are multiple organizations, separate them with commas.
```{text}```
"""
!wget --user-agent="Mozilla" https://arxiv.org/pdf/2104.12871.pdf -O /tmp/mitchell.pdf -q
df = extractor.apply(prompt, fpath='/tmp/mitchell.pdf', pdf_pages=[1], stop=['\n'])
df.loc[df['Extractions'] != 'NA'].Extractions[0]

# OUTPUT
# Santa Fe Insitute
```

## Data Cleaning

```python
prompt = """Correct the grammar and spelling in the supplied sentences.  Here are some examples.
[Sentence]:
I love goin to the beach.
[Correction]: I love going to the beach.
[Sentence]:
Let me hav it!
[Correction]: Let me have it!
[Sentence]:
It have too many drawbacks.
[Correction]: It has too many drawbacks.
[Sentence]:
I do not wan to go
[Correction]:"""
saved_output = llm.prompt(prompt, stop=['\n\n'])

# OUTPUT
#  I do not want to go.
```

We find prompts like the above to be particularly useful in fixing OCR errors prior to running text through an LLM.

## Text Generation

```python
prompt = """Generate a tweet based on the supplied Keyword. Here are some examples.
[Keyword]:
markets
[Tweet]:
Take feedback from nature and markets, not from people
###
[Keyword]:
children
[Tweet]:
Maybe we die so we can come back as children.
###
[Keyword]:
startups
[Tweet]:
Startups should not worry about how to put out fires, they should worry about how to start them.
###
[Keyword]:
climate change
[Tweet]:"""

saved_output = llm.prompt(prompt)

# OUTPUT
#  Climate change is not a problem for our grandchildren to solve, it's a problem for us to solve for our grandchildren. #actonclimate #climateaction
```

## Question-Answering

The tookit comes pre-packages with a vector database (Chroma as of this writing).  This allows you ask questions about a set of documents and generate answers with sources.

```python
llm.ingest("./sample_data/")
result = llm.ask("What is ktrain?")

# OUTPUT
#  Ktrain is a low-code machine learning library that facilitates the full machine learning workflow
# from curating and preprocessing inputs to training, tuning, troubleshooting, and applying models.
# It is well-suited for domain experts who may have less experience with machine learning and software coding.
```

## Document Summarization

By insantiating a `Summarizer` object, you can summarize a document by pointing directly to the file:

```python
summarizer = Summarizer(llm)
summary = summarizer.summarize('ktrain.pdf')
```


For more information, please see the f[ull documentation](https://amaiya.github.io/onprem/).


